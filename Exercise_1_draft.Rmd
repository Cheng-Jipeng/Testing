---
title: "Exercise - 1"
author: "Jipeng Cheng"
date: "2/8/2022"
output: bookdown::html_document2
---

```{r setup, include=FALSE, cache=TRUE}
knitr::opts_chunk$set(echo = FALSE)
```

# Data visualization: flights at ABIA
## Worst Destination Airports
What are the bad airports to fly to if you want to depart from Austin? We measure how "bad" these airports are with the proportion of delayed or cancelled flights. Four levels of flights' states are defined as:

- On time: no delays
- Minor delay: arrival delay < 30 mins
- Major delay: arrival delay > 30 min
- Cancelled: flights being cancelled

Personally, we focus on flights being cancelled or with arrival delays larger than 30 minutes, which are extremely annoying to travelers. The "Bad Index" with which we rate destination airports are formally given by
$$
\text{Bad Index}_{\text{Destination}} = \frac{\text{# of major delay + # of cancelled}}{\text{# from AUS to Destination}},
$$
where # refers to the number of flights. 

The *yearly top 10 worst destination airports* of 2008 together with their "Bad Index" are shown in Figure \@ref(fig:fig1). **Newark airport** is the worst airport given our rating system: the flights from Austin to Newark are most likely to be delayed by over 30 minutes or cancelled. The second follows **JFK airport** in New York. Considering that EWR also serves The NYC Metropolitan Area, it seems that choosing **New York** as destination is the real cause. Busy city and busy airports!
```{r fig1, fig.align='center', fig.cap= "Top 10 Worst Destination of 2008", message = FALSE}
# Remember to reset working directory
library(tidyverse)
library(mosaic)
library(dplyr)
library(stringr)
library(data.table)
library(lubridate)
library(ggplot2)
library(tidyverse)
library(ggplot2)
library(rsample)  
library(caret)
library(modelr)
library(parallel)
library(foreach)
library(ggpubr)
library(kableExtra)
library(ggrepel)
abia = read.csv("/Users/jipengcheng/Library/Mobile Documents/com~apple~CloudDocs/【MA】Course/Sp_Data Mining/ECO395M/data/ABIA.csv")

# Plot column graph of Bad Index vs. Top 10 worst destination
bad_airports_top10 = abia %>%
  filter(Origin == "AUS") %>%  
  group_by(Dest) %>%
  summarize(cancelled = sum(Cancelled, na.rm=TRUE),
            major_delay = sum(ArrDelay > 30, na.rm=TRUE),
            cancel_ratio = sum(Cancelled, na.rm=TRUE)/n(),
            major_delay_ratio = sum(ArrDelay > 30, na.rm=TRUE)/n(),
            entry_count = n()) %>%
  mutate(bad_count = cancelled + major_delay,
         bad_ratio = cancel_ratio + major_delay_ratio) %>%
  filter(bad_count > 1) %>%
  arrange(desc(bad_ratio)) %>%
  head(10)

bad_airports_top10 %>%
  ggplot(aes(fct_reorder(Dest, -bad_ratio),
             bad_ratio))+
  geom_col() +
  labs(y = "Bad Index", x= "Destination Airports")
```

We can also visualize the top 10 airports on a U.S. map in Figure \@ref(fig:fig2) with blue bar refering to the values of their "Bad Index".
```{r fig2, fig.align='center', fig.cap= "Top 10 Worst Destination of 2008 on Map"}
# Visualize bad index on map
airports_coord_raw = read.csv("https://raw.githubusercontent.com/datasets/airport-codes/master/data/airport-codes.csv")
## Generate top 10 dest names
airports_name = abia %>%
  select(Dest) %>%
  unique() %>%
  filter(Dest != "AUS")
## Get their coordinates from raw data
airports_coord = airports_coord_raw %>%
  filter(iata_code %in% airports_name$Dest) %>%
  select(iata_code, coordinates) %>%
  separate(coordinates, into=c("lon", "lat"), sep=",") # Best way to split string data
## Recreate top 10 table by adding coordinates
bad_airports_top10 = merge(bad_airports_top10, airports_coord, by.x = "Dest", by.y = "iata_code") %>%
  arrange(desc(bad_ratio))
bad_airports_top10$lon = as.numeric(bad_airports_top10$lon)
bad_airports_top10$lat = as.numeric(bad_airports_top10$lat)
## Plot map
usamap <- borders("usa", colour="#efede1", fill="#efede1") 
options(ggrepel.max.overlaps = Inf)
ggplot(bad_airports_top10) + usamap +
  geom_point(aes(x = lat, y = lon), col = "#970027") +
  geom_text_repel(aes(x = lat, y = lon, label = Dest)) + 
  geom_errorbar(aes(x = lat, ymin=lon, ymax=lon+exp(10 * bad_ratio) - 1), color = "blue") +
  theme_void()
```

If you are also interested in if there are many minor delays in these bad flights, Figure \@ref(fig:fig3) shows the proportions of all kinds of flights' states. If *minor delays can annoy you very much too*, then you must avoid choosing **Atlanta airport** as your destination among the top 10 worst airports.

```{r fig3, fig.align='center', fig.cap= "Distribution of Delays in Top 10 Worst Destination of 2008"}
# Plot percent stacked barchart (Assign multiple values to 1 variable with ifelse)
bad_airports_plot = abia %>%
  filter(Dest %in% bad_airports_top10$Dest) %>%
  mutate(bad = ifelse(Cancelled == 1, "Cancelled", 
                      ifelse( ArrDelay <= 30 & ArrDelay>0, "Minor Delay",
                              ifelse(ArrDelay >30, "Major Delay", "On Time")))) %>%
  group_by(Dest) %>%
  mutate(order_count = sum(Cancelled == 1 | ArrDelay > 30, na.rm=TRUE)/n()) %>%
  arrange(desc(order_count))

bad_airports_plot %>%
  drop_na(bad) %>%
ggplot(aes(reorder(Dest, -order_count), 1, fill = factor(bad, levels=c( 
  "On Time","Minor Delay", "Major Delay","Cancelled")))) + 
  geom_bar(position="fill", stat="identity") +
  labs(y = "Proportions of Each Kind of Delays", x= "Destination Airports", fill =" ")
```

Does the top list change over the months in 2008? Table \@ref(tab:tab1) answers the question in details by focusing on the top 5 worst airports in each month. Definitely there are new blood across different months like Boston airport in February, Ontario airport in April.  

```{r tab1}
bad_airports_bymonth = abia %>%
  filter(Origin == "AUS") %>%
  group_by(Dest, Month) %>%
  summarize(bad_ratio = sum(Cancelled == 1 | ArrDelay > 30, na.rm=TRUE)/n(),
            bad_count = sum(Cancelled == 1 | ArrDelay > 30, na.rm=TRUE),
            .groups = 'drop') %>%
  filter(bad_count > 1) 

bad_airports_bymonth_top5 = bad_airports_bymonth %>%
  group_by(Month) %>%
  mutate(bad_rank = rank(-bad_ratio, ties.method = "first")) %>%
  slice_max(order_by = bad_ratio, n=5) %>%
  filter(bad_rank < 6)

by_month_table = bad_airports_bymonth_top5 %>%
  summarize(First = Dest[bad_rank ==1],
            Second = Dest[bad_rank ==2],
            Third = Dest[bad_rank ==3],
            Fourth = Dest[bad_rank ==4],
            Fifth = Dest[bad_rank ==5])

by_month_table %>%
  kbl(caption = "Top 5 Worst Destinations across Months") %>%
  kable_styling()
```

A more sketchy way to capture the variability is counting how many time airports occur in the monthly lists of top 5 worst. This is given by Figure \@ref(fig:fig4). There are 23 airports in all of the monthly lists, and this also support our conclusion from yearly list: all 12 lists contains EWR and 6 lists contains JFK.

```{r fig4, fig.align='center', fig.cap= "Frequency of Appearances in Top 5 Worst Destinations across Months"}
bad_airports_bymonth_top5 %>%
  group_by(Dest) %>%
  mutate(Dest_count = n()) %>%
  ggplot(aes(x=reorder(Dest, Dest_count)))+
  geom_bar() +
  labs(y = "Counts of Appearances", x="Destination Airports") + 
  coord_flip() 
  
```

## Best Month to Depart from AUS
According to the following graph, it shows that September has the lowest delay rate in 2008.
```{r fig5, fig.align='center', fig.cap="Delay Rate by Month in 2008"}
abia_modified = abia %>%
  mutate(if_delay = ifelse(DepDelay > 0, 1, 0))

delay_month = abia_modified %>%
  filter(Origin == 'AUS', Cancelled == 0) %>%
  group_by(Month)%>%
  summarise(total_count = n(), delay_num =sum(if_delay), delay_rate = delay_num/total_count)

ggplot(data = delay_month) + 
  geom_line(mapping = aes(x=Month, y=delay_rate),color = "red") +
  scale_x_continuous(breaks = 1:12) +
  labs(y="Delay Rate", x = "Month")
```


# Wrangling the Billboard Top 100
## Part A
```{r tab2}
billboard = read.csv("/Users/jipengcheng/Library/Mobile Documents/com~apple~CloudDocs/【MA】Course/Sp_Data Mining/ECO395M/data/billboard.csv")
# Part A: A table of the top 10 most popular songs since 1958
top_song_table = billboard %>%
  filter(year >= 1958) %>%
  group_by(performer, song) %>%
  summarize(count = n(), .groups = 'drop') %>%
  arrange(desc(count)) %>%
  head(10)

top_song_table %>%
  knitr::kable(caption = "Top 10 Most Popular Songs Since 1958",
               col.names = c("Performer","Song","Count")) %>%
  kable_styling()
```
## Part B
```{r fig6, fig.align='center', fig.cap= "Musical Diversity Over Time"}
# Part B
billboard %>%
  filter(year != 1958 & year != 2021) %>%
  group_by(year) %>%
  summarise(musical_diversity = length(unique(song))) %>%
  ggplot(aes(x = year, y = musical_diversity))+
  geom_line()+
  geom_point()+
  labs(x = "Year", y = "# of Unique Songs Appearing in the Billboard Top 100 over Years")
```
## Part C
```{r fig7, fig.align='center', fig.cap="Artists Having over 30 Ten-Week-Hits Songs"}
# Part C
ten_week_hit = billboard %>%
  group_by(performer, song) %>%
  summarize(count = n(), .groups='drop') %>%
  filter(count >= 10)

ten_week_hit_30songs = ten_week_hit%>%
  group_by(performer) %>%
  summarize(week_hit_count = n()) %>%
  filter(week_hit_count >= 30)

ten_week_hit_30songs %>%
  ggplot(aes(x=fct_reorder(performer, week_hit_count), y=week_hit_count)) +
  geom_col() +
  coord_flip()+
  labs(x="Weeks of Hits", y="Artists")
```

# Wrangling the Olympics
## Part A
```{r}
olympics_top20 = read.csv("/Users/jipengcheng/Library/Mobile Documents/com~apple~CloudDocs/【MA】Course/Sp_Data Mining/ECO395M/data/olympics_top20.csv")
olympics_top20 %>% 
  filter(sport == "Athletics", sex == "F") %>%
  summarize(q95_height = quantile(height, 0.95))
```
The 95th percentile of heights for female competitors across all Athletics events is **183cm**.

## Part B
```{r}
# Part B
greatest_var = olympics_top20 %>%
  filter(sex == "F") %>%
  group_by(event) %>%
  summarize(height_variability = sd(height)) %>%
  arrange(desc(height_variability)) %>%
  head(1)

greatest_var %>%
  knitr::kable(col.names = c("Event","Height Variability")) %>%
  kable_styling(full_width = F)
```
It is shown that women's coxed four rowing had the greatest variability in competitor's heights across the entire history of the Olympics

## Part C
```{r, fig.align='center', fig.cap="Increasing Trend of Age Similar Across Female and Male Swimmers' Age After Female Participation in Olympics"}
# Part C
olympics_top20 %>%
  filter(sport == "Swimming") %>%
  group_by(sex, year) %>%
  summarize(avg_age = mean(age), .groups='drop') %>%
  ggplot(aes(x = year ,y = avg_age, color = sex)) +
  geom_line() +
  geom_point() +
  labs(x="Year", y="Average Age")
```

# K-nearest neighbors
```{r fig8, fig.align='center', fig.cap="RMSE vs. K", warning = FALSE}
sclass = read.csv("/Users/jipengcheng/Library/Mobile Documents/com~apple~CloudDocs/【MA】Course/Sp_Data Mining/ECO395M/data/sclass.csv")
K_folds = 5
k_grid = rep(1:125)
# For trim = 350
sclass_350 = sclass %>% filter(trim == "350")
sclass_350_folds = crossv_kfold(sclass_350, k=K_folds)
cv_grid_350 = foreach(k = k_grid, .combine='rbind') %dopar% {
  models = map(sclass_350_folds$train, ~ knnreg(price ~ mileage, k=k, data = ., use.all=FALSE))
  errs = map2_dbl(models, sclass_350_folds$test, modelr::rmse)
  c(k=k, err = mean(errs), std_err = sd(errs)/sqrt(K_folds))
} %>% as.data.frame

k_min_rmse_350 = cv_grid_350 %>%
  slice_min(err) %>%
  pull(k)
  
k_plot_350 = ggplot(cv_grid_350) +
  ggtitle("RMSE of Prediction for 350's vs. K") +
  geom_point(aes(x = k, y = err)) +
  geom_errorbar(aes(x = k, ymin = err-std_err, ymax = err+std_err)) + 
  geom_vline(aes(xintercept = k_min_rmse_350)) +
  labs(x="K", y="RMSE for 350's ")
## predictions vs. x
sclass_350_split = initial_split(sclass_350, prop = 0.8)
sclass_350_train = training(sclass_350_split)
sclass_350_test = testing(sclass_350_split)
knn_optimal_350 = knnreg(price ~ mileage, data = sclass_350_train, k = k_min_rmse_350)
pred_vs_x_350= sclass_350_test %>%
  mutate(price_predict = predict(knn_optimal_350, sclass_350_test)) %>%
  ggplot()+
  ggtitle("Prediction for 350's with Optimal K") +
  geom_point(aes(x=mileage, y=price))+
  geom_line(aes(x=mileage, y=price_predict))


# For trim = 65
sclass_65= sclass %>% filter(trim == "65 AMG")
sclass_65_folds = crossv_kfold(sclass_65, k=K_folds)
cv_grid_65 = foreach(k = k_grid, .combine='rbind') %dopar% {
  models = map(sclass_65_folds$train, ~ knnreg(price ~ mileage, k=k, data = ., use.all=FALSE))
  errs = map2_dbl(models, sclass_65_folds$test, modelr::rmse)
  c(k=k, err = mean(errs), std_err = sd(errs)/sqrt(K_folds))
} %>% as.data.frame

k_min_rmse_65 = cv_grid_65 %>%
  slice_min(err) %>%
  pull(k)

k_plot_65 = ggplot(cv_grid_65) +
  ggtitle("RMSE of Prediction for 65 AMG's vs. K") +
  geom_point(aes(x = k, y = err)) +
  geom_errorbar(aes(x = k, ymin = err-std_err, ymax = err+std_err)) +
  geom_vline(aes(xintercept = k_min_rmse_65)) +
  labs(x="K", y="RMSE")
## predictions vs. x
sclass_65_split = initial_split(sclass_65, prop = 0.8)
sclass_65_train = training(sclass_65_split)
sclass_65_test = testing(sclass_65_split)
knn_optimal_65 = knnreg(price ~ mileage, data = sclass_65_train, k = k_min_rmse_65)
pred_vs_x_65 = sclass_65_test %>%
  mutate(price_predict = predict(knn_optimal_65, sclass_65_test)) %>%
  ggplot()+
  ggtitle("Prediction for 65 AMG's with Optimal K") +
  geom_point(aes(x=mileage, y=price))+
  geom_line(aes(x=mileage, y=price_predict))

# Combine multiple plots
ggarrange(k_plot_350, k_plot_65, 
          ncol = 1, nrow = 2)
```
Thus, the graphs show that the optimal K for predicting 350's prices with KNN method should be
```{r}
k_min_rmse_350
```
and the optimal K for predicting 65 AMG's price with KNN method should be
```{r}
k_min_rmse_65
```
given they yield the smallest RMSEs respectively.
```{r fig9, fig.align='center', fig.cap="Prediction vs. Testing Set"}
ggarrange(pred_vs_x_350, pred_vs_x_65, 
          ncol = 1, nrow = 2) 
```
The trim of 350 tends to yield a larger optimal value of K. This might be because the sample size of 350's is larger and allows a bigger K to capture more information, reduce estimation bias, and avoid being heavily affected by noises.







